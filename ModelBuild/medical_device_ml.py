# -*- coding: utf-8 -*-
"""Medical_Device_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zOzqYfpyPhYqZO57uth1Clf3lRFFg3ED

####Disclaimer : Full ML System design and full dataset were excluded to protect company IP
"""

# This will help in making the Python code more structured automatically (good coding practice)
# %load_ext nb_black

# Libraries to help with reading and manipulating data
import pandas as pd
import numpy as np

# Libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# To tune model, get different metric scores, and split data
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    roc_auc_score,
    ConfusionMatrixDisplay,
)
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score

# To be used for data scaling and one hot encoding
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder

# To impute missing values
from sklearn.impute import SimpleImputer
from sklearn import metrics

# To oversample and undersample data
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# To do hyperparameter tuning
from sklearn.model_selection import RandomizedSearchCV

# To define maximum number of columns to be displayed in a dataframe
pd.set_option("display.max_columns", None)

# To supress scientific notations for a dataframe
pd.set_option("display.float_format", lambda x: "%.3f" % x)

# To help with model building
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    AdaBoostClassifier,
    GradientBoostingClassifier,StackingClassifier

)
from xgboost import XGBClassifier


# To suppress scientific notations
pd.set_option("display.float_format", lambda x: "%.3f" % x)

# To supress warnings
import warnings

warnings.filterwarnings("ignore")

"""##**Reading the dataset**"""

#connecting to google drive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Previous_Projects/medical_device_failure.csv")

# copying data to another variable to avoid any changes to original data
data = df.copy()
list(data.columns)

"""#### Check for missing values"""

data.isna().sum()

data['Device_Fail'].value_counts(normalize = True)

"""- To prevent data leakage, missing value will be treated after train-test split

##**Data Preparation for model build**
"""

# Separating features and the target column
X = data.drop('Device_Fail',axis=1)

y = data['Device_Fail'].apply(lambda x : 1 if x=='Y' else 0)

# Splitting data into training, validation and test set:
# first we split data into 2 parts, say temporary and test

X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1, stratify=y
)

# then we split the temporary set into train and validation

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=1, stratify=y_temp
)
print(X_train.shape, X_val.shape, X_test.shape)

print("Number of instances in train set :", X_train.shape[0])
print("Number of instances in validation set =", X_val.shape[0])
print("Number of instances in test set =", X_test.shape[0])

"""#### **Treatment of Missing Values**"""

from sklearn.impute import SimpleImputer
#select numeric variables of integer and float type
numeric_var = list(X_train.select_dtypes(include=['float','int']).columns)

# Impute numerical columns
numeric_imputer = SimpleImputer(strategy='median') #use mean if data is from a gaussian distribution otherwise you use the median
X_train[numeric_var] = numeric_imputer.fit_transform(X_train[numeric_var])
X_val[numeric_var] = numeric_imputer.transform(X_val[numeric_var])
X_test[numeric_var] = numeric_imputer.transform(X_test[numeric_var])

# Checking that no column has missing values in train, validation and test sets
print(X_train.isna().sum())
print("~" * 50)
print(X_val.isna().sum())
print("~" * 50)
print(X_test.isna().sum())
print("~" * 50)

"""###**Dummy Features**

- As variables are added, the data space becomes increasingly sparse, and classification and prediction models fail because the available data are insufficient to provide a useful model across so many variables.  

- The difficulties posed by adding a variable increase exponentially with the addition of each variable.

- Dimensionality is the number of predictors or input variables in a model, and the “curse” refers to the problems that result from including too many features (predictor variables) in a model.
"""

X_train = pd.get_dummies(X_train, drop_first=True)
X_val = pd.get_dummies(X_val, drop_first=True)
X_test = pd.get_dummies(X_test, drop_first=True)
print(X_train.shape, X_val.shape, X_test.shape)

"""##**Strategy for Algorithm Development**

- Build 3 boosted trees models and stack them together on the train data and observe their performance on test
  
  - `Adaboost`, `GBM` and `XGBoost`

- use `randomizedSearchCV` for model optimization
- Use a user defined function to consolidate all offline performance metrics such precision, recall, F1 measure.

###**Selection of`Anchor` Metric to Evaluate Algorithm performance**


**Loss of Resources Vs Loss of Opportunity tradeoff:**

1. Predicting the device will fail (Y), but in reality, it does not fail (N) - Loss of resources

2. Preidicting the device will not fail (N), but in reality it does fail (Y)  - Loss of opportunity

* Predicting a device will not fail but the device fails

**Strategy to reduce this loss i.e need to reduce False Negatives?**
* maximize `Recall`
**Impact*  Greater `Recall` reduces the likelihood of false negatives.

* **`Recall` is the anchor metric to evaluate model performance**

### **Define and aggregate all performance metrics**
"""

# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1,},
        index=[0],
    )

    return df_perf

def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

"""###**Model Build Iteration 0**"""

models = []  # Empty list to store all the models

# Appending models into the list
models.append(("GBM", GradientBoostingClassifier(random_state=15)))
models.append(("Adaboost", AdaBoostClassifier(random_state=15)))
models.append(("XGB", XGBClassifier(random_state=15)))

print("\nTraining Performance:\n")
for name, model in models:
    model.fit(X_train, y_train)
    scores = recall_score(y_train, model.predict(X_train))
    print("{}: {}".format(name, scores))

print("\nValidation Performance:\n")
for name, model in models:
    model.fit(X_train, y_train)
    scores_val = recall_score(y_val, model.predict(X_val))
    print("{}: {}".format(name, scores_val))

print("\nTraining and Validation Performance Difference:\n")

for name, model in models:
    model.fit(X_train, y_train)
    scores_train = recall_score(y_train, model.predict(X_train))
    scores_val = recall_score(y_val, model.predict(X_val))
    delta1 = scores_train - scores_val
    print("{}: Train_score: {:.2f}, Val_score: {:.2f}, Delta: {:.2f}".format(name, scores_train, scores_val, delta1))

"""- observation: `Xgboost` is severely overfitting, `Adaboost` is struggling to understand the underlying patterns in both train and test sets. `GBM` is borderline overfitting.

- To address this, data augmentation technique `undersampling` will be used

#### **Model Build Iteration 1**
####**Data Augmentation - Undersampling of Majority Class**
"""

rus = RandomUnderSampler(random_state=15)
X_train_un, y_train_un = rus.fit_resample(X_train, y_train)

print("Undersampling,dimension of train_X: {}".format(X_train_un.shape))
print("Undersampling,dimension of train_y: {}".format(y_train_un.shape))

models = []  # Empty list to store all the models

# Appending models into the list

models.append(("GBM", GradientBoostingClassifier(random_state=15)))
models.append(("Adaboost", AdaBoostClassifier(random_state=15)))
models.append(("XGB", XGBClassifier(random_state=15)))


print("\n" "Training Performance:" "\n")
for name, model in models:
    model.fit(X_train_un, y_train_un)
    scores = recall_score(y_train_un, model.predict(X_train_un))
    print("{}: {}".format(name, scores))

print("\n" "Validation Performance:" "\n")

for name, model in models:
    model.fit(X_train_un, y_train_un)
    scores = recall_score(y_val, model.predict(X_val))
    print("{}: {}".format(name, scores))

print("\nTraining and Validation Performance Difference:\n")

for name, model in models:
    model.fit(X_train_un, y_train_un)
    scores_train = recall_score(y_train_un, model.predict(X_train_un))
    scores_val = recall_score(y_val, model.predict(X_val))
    delta3 = scores_train - scores_val
    print("{}: Train_score: {:.2f}, Val_score: {:.2f}, Delta: {:.2f}".format(name, scores_train, scores_val, delta3))

"""Observation : After undersampling - we have been able to address the models inability to generalize on validation set.

- We can optimize the models to see if we can improve performance

- caveat: Hyperparameterization does not gaurantee model improvement, it's a hit or miss

#### **Model Build Iteration 2**

### Optimization : Hyperparameter Tuning
"""

# Choose the type of classifier.
xodel = AdaBoostClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {'n_estimators': np.arange(20,120,10),
              'learning_rate': [1, 0.1, 0.001, 0.01],
              }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=xodel, param_distributions=parameters, n_jobs = -1, n_iter=50, scoring=scorer, cv=5, random_state=15)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_un, y_train_un)

print("Best hyperparameter combination are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

ada_classifier_tuned = AdaBoostClassifier(
    random_state=15,
    n_estimators=50,
    learning_rate=0.1
    )
ada_classifier_tuned.fit(X_train_un, y_train_un)

ada_classifier_tuned_model_train_perf = model_performance_classification_sklearn(ada_classifier_tuned, X_train_un, y_train_un)
print("Training performance \n",ada_classifier_tuned_model_train_perf)

ada_classifier_tuned_model_val_perf = model_performance_classification_sklearn(ada_classifier_tuned, X_val, y_val)
print("Validation performance \n",ada_classifier_tuned_model_val_perf)

"""###**Hyperparameter Tuning of Gradient Boosting Classifier using undersampled train set**"""

#Creating pipeline
xodel = GradientBoostingClassifier(random_state=15)

#Parameter grid to pass in RandomSearchCV
parameters = {

    "n_estimators": np.arange(10,125,15),
    "learning_rate": [0.01, 0.1, 0.005, 1],
    "subsample":[0.6,0.8,1],
    "max_features":[0.6,0.8,0.9],
}

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=xodel, param_distributions=parameters, n_iter=50, scoring=scorer, cv=5, random_state=15, n_jobs = -1)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_un,y_train_un)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

gbm_classifier_tuned = GradientBoostingClassifier(
    random_state=15,
    subsample=0.6,
    n_estimators=70,
    max_features=0.9,
    learning_rate=0.1
)
gbm_classifier_tuned.fit(X_train_un, y_train_un)

# Checking model's performance on training set
gbm_classifier_train = model_performance_classification_sklearn(
    gbm_classifier_tuned, X_train_un, y_train_un
)
gbm_classifier_train

# Checking model's performance on training set
gbm_classifier_val = model_performance_classification_sklearn(
    gbm_classifier_tuned, X_val, y_val
)
gbm_classifier_val

"""###**Hyperparameter Tuning of XGBClassifier using undersampled train set**"""

xodel = XGBClassifier(random_state=1,eval_metric='logloss')

#Parameter grid to pass in RandomSearchCV
parameters={'n_estimators':np.arange(15,135,15),
            'scale_pos_weight':[1,2,4],
            'learning_rate':[0.01,0.001,0.05,0.1],
            'gamma':[1,3],
            'subsample':[0.5,0.8]
           }

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=xodel, param_distributions=parameters, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=15)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_un,y_train_un)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

xgb_classifier_tuned = XGBClassifier(
    random_state=1,

    subsample=0.5,
    scale_pos_weight=2,
    n_estimators=30,
    learning_rate=0.01,
    gamma=3,
)
xgb_classifier_tuned.fit(X_train_un, y_train_un)

# Checking model's performance on training set
xgb_classifier_train = model_performance_classification_sklearn(xgb_classifier_tuned, X_train_un, y_train_un)
xgb_classifier_train

# Checking model's performance on val set
xgb_classifier_val = model_performance_classification_sklearn(xgb_classifier_tuned, X_val, y_val)
xgb_classifier_val

"""###**Stacking Model**

let's build a stacking model with the tuned models - `adaboost`,`XGB`, then use `GBM` to get the final prediction.
"""

estimators=[('adaboost', AdaBoostClassifier(random_state=15)),('XGB', XGBClassifier(random_state=15))]
final_estimator= GradientBoostingClassifier(random_state=15)

stacking_classifier =StackingClassifier(estimators=estimators, final_estimator=final_estimator,cv=5)
stacking_classifier.fit(X_train_un,y_train_un)

"""###**XGBoost Classifier**"""

stacking_classifier_model_train_perf = model_performance_classification_sklearn(stacking_classifier, X_train_un, y_train_un)
print("Training performance \n",stacking_classifier_model_train_perf)

stacking_classifier_model_test_perf = model_performance_classification_sklearn(stacking_classifier, X_test, y_test)
print("Test performance \n",stacking_classifier_model_test_perf)

"""### Observation : Per Recall score, undersampling of the train_set improved model performance

#### **Next step: Identify key predictors driving device failure - Feature Importance**
"""

feature_names = X_train.columns
importances = gbm_classifier_tuned.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(10, 8))
plt.title("Feature Importance")
plt.barh(range(len(indices)), importances[indices], color="green", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Information Gain")
plt.show()

"""####**Observation** `Torque`, `Velocity` and `Wear_Measure` are the key features driving device failure

###**Retrieval of prediction**
"""

pred = gbm_classifier_tuned.predict(X_test)

y_hats = pd.DataFrame(pred, columns =['y_hats'])
y_hats.sample(10)